{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"PCA_practice.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"SUxgxAyHEaP-","colab_type":"text"},"source":["## Визуализация данных"]},{"cell_type":"markdown","metadata":{"id":"7HOfME-sEaQA","colab_type":"text"},"source":["На прошлом семинаре мы рассмотрели полный путь решения задачи от визуализации и предобработки до стекинга и блендинга. Пройдем его ещё раз сосредоточившись на визуализации и отборе признаков.\n","\n","Понижение размерности (визуализация в 2D это частный случай понижения размерности до 2-х измерений) можно использовать и для других целей:\n","\n","* Сокращение ресурсоемкости алгоритмов\n","* Ослабление влияния проклятия размерности и тем самым уменьшение переобучения\n","* Переход к более информативным признакам"]},{"cell_type":"markdown","metadata":{"id":"mxQCcDXIEaQB","colab_type":"text"},"source":["## Отбор признаков"]},{"cell_type":"code","metadata":{"id":"sLlPSDyJEaQC","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37Ew_CD0EaQG","colab_type":"text"},"source":["## Метод главных компонент (Principal Component Analysis, PCA)\n","\n","Выделение новых признаков путем их отбора часто дает плохие результаты, и\n","в некоторых ситуациях такой подход практически бесполезен. Например, если\n","мы работаем с изображениями, у которых признаками являются яркости пикселей,\n","невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о\n","содержимом картинки. \n","\n","Поэтому признаки нужно как-то комбинировать. Рассмотрим метод главных компонент.\n","\n","Этот метод делает два важных упрощения задачи\n","\n","1. Игнорируется целевая переменная\n","2. Строится линейная комбинация признаков\n","\n","П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является\n","таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то\n","внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с\n","целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n","\n","П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него."]},{"cell_type":"markdown","metadata":{"id":"vbaiwGADEaQH","colab_type":"text"},"source":["### Теория\n","\n","Кратко вспомним, что делает этот метод (подробно см. в лекции).\n","\n","Обозначим $X$ - матрица объекты-признаки, с нулевым средним каждого признака,\n","а $w$ - некоторый единичный вектор. Тогда\n","$Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор,\n","который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию\n","вдоль этого направления):\n","\n","$$\n","    \\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w\n","$$\n","\n","Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным\n","значением. После этого все пространство проецируется на ортогональное дополнение к вектору\n","$w$ и процесс повторяется."]},{"cell_type":"markdown","metadata":{"id":"Vms0MRh9EaQI","colab_type":"text"},"source":["### PCA на плоскости\n","\n","Для начала посмотрим на метод PCA на плоскости для того, чтобы\n","лучше понять, как он устроен.\n","\n","Попробуем специально сделать один из признаков более значимым и проверим, что PCA это обнаружит. Сгенерируем выборку из двухмерного гауссовского распределения. Обратите внимание, что выборка\n","изначально выбирается центрированной."]},{"cell_type":"code","metadata":{"id":"_8OKhmheEaQI","colab_type":"code","colab":{}},"source":["np.random.seed(314512)\n","\n","data_synth_1 = np.random.multivariate_normal(\n","    mean=[0, 0], \n","    cov=[[4, 0], \n","         [0, 1]],\n","    size=1000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmFvBp6XEaQL","colab_type":"text"},"source":["Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент.\n","В результате работы PCA из sklearn в `dec.components_` будут лежать главные направления (нормированные), а в `dec.explained_variance_` - дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их\n","значимости."]},{"cell_type":"code","metadata":{"id":"EUBaoV5MEaQL","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","\n","\n","def PCA_show(dataset):\n","    plt.scatter(*zip(*dataset), alpha=0.5)\n","    \n","    dec = PCA()\n","    dec.fit(dataset)\n","    ax = plt.gca()\n","    for comp_ind in range(dec.components_.shape[0]):\n","        component = dec.components_[comp_ind, :]\n","        var = dec.explained_variance_[comp_ind]\n","        start, end = dec.mean_, component * var\n","        ax.arrow(start[0], start[1], end[0], end[1],\n","                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n","    \n","    ax.set_aspect('equal', adjustable='box')\n","\n","plt.figure(figsize=(16, 8))\n","PCA_show(data_synth_1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mf7r4YPEaQP","colab_type":"text"},"source":["Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав\n","дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим,\n","что для PCA это ничего не изменит."]},{"cell_type":"code","metadata":{"id":"FdR6GwB_EaQQ","colab_type":"code","colab":{}},"source":["angle = np.pi / 6\n","rotate = np.array([\n","        [np.cos(angle), - np.sin(angle)],\n","        [np.sin(angle), np.cos(angle)],\n","    ])\n","data_synth_2 = rotate.dot(data_synth_1.T).T\n","plt.figure(figsize=(16, 8))\n","PCA_show(data_synth_2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJDhXCfzEaQS","colab_type":"text"},"source":["Ну вот, все нормально. \n","\n","Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки).\n","\n","**Упражнение.** Объясните, почему так произошло."]},{"cell_type":"markdown","metadata":{"id":"V_SvGFl_GafU","colab_type":"text"},"source":["### По [этой ссылке](http://setosa.io/ev/principal-component-analysis/) можно поиграться с понижением размерности трезмерных данных в интерактивном режиме - крайне рекомендую это сделать"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"v9qnybskEaQT","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_circles, make_moons, make_blobs\n","\n","np.random.seed(54242)\n","data_synth_bad = [\n","    make_circles(n_samples=1000, factor=0.2, noise=0.1)[0]*2,\n","    make_moons(n_samples=1000, noise=0.1)[0]*2,\n","    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n","    np.random.multivariate_normal(\n","        mean=[0, 1.5], \n","        cov=[[3, 1], \n","             [1, 1]],\n","        size=1000),\n","]\n","\n","\n","plt.figure(figsize=(16,8))\n","rows, cols = 2, 2\n","for i, data in enumerate(data_synth_bad):\n","    plt.subplot(rows, cols, i + 1)\n","    PCA_show(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ML5BlI3aEaQV","colab_type":"text"},"source":["### Лица людей\n","\n","Рассмотрим датасет с фотографиями лиц людей и применим к его признакам PCA.\n","\n","Ниже изображены примеры лиц из базы, и последняя картинка - это \"среднее лицо\".\n","\n","Подробнее о датасете можно прочитать [здесь](https://scikit-learn.org/0.22/datasets/index.html#the-olivetti-faces-dataset)\n","\n","По 10 фотографий для 40 людей"]},{"cell_type":"code","metadata":{"id":"-TVrElk6EaQW","colab_type":"code","colab":{}},"source":["from sklearn.datasets import fetch_olivetti_faces\n","\n","faces = fetch_olivetti_faces(shuffle=True, random_state=432542)\n","faces_images = faces.data\n","faces_ids = faces.target\n","image_shape = (64, 64)\n","    \n","mean_face = faces_images.mean(axis=0)\n","\n","plt.figure(figsize=(16, 8))\n","rows, cols = 2, 4\n","n_samples = rows * cols\n","for i in range(n_samples - 1):\n","    plt.subplot(rows, cols, i + 1)\n","    plt.imshow(faces_images[i, :].reshape(image_shape), interpolation='none',\n","               cmap='gray')\n","    plt.xticks(())\n","    plt.yticks(())\n","    \n","plt.subplot(rows, cols, n_samples)\n","plt.imshow(mean_face.reshape(image_shape), interpolation='none',\n","           cmap='gray')\n","plt.xticks(())\n","_ = plt.yticks(())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHU3wCTlEaQb","colab_type":"text"},"source":["Теперь найдем главные компоненты"]},{"cell_type":"code","metadata":{"id":"sEz4g5okEaQc","colab_type":"code","colab":{}},"source":["pca_reduction = PCA()\n","faces_images -= mean_face\n","pca_reduction.fit(faces_images)\n","\n","plt.figure(figsize=(16, 8))\n","rows, cols = 2, 4\n","n_samples = rows * cols\n","for i in range(n_samples):\n","    plt.subplot(rows, cols, i + 1, title=f'{i}я главная компонента')\n","    plt.imshow(pca_reduction.components_[i, :].reshape(image_shape), interpolation='none',\n","               cmap='gray')\n","    plt.xticks(())\n","    plt.yticks(())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dO-Da7IWEaQe","colab_type":"text"},"source":["Получилось жутковато, что уже неплохо, но есть ли от этого какая-то польза?\n","\n","Да, польза есть, увидим это ниже."]},{"cell_type":"markdown","metadata":{"id":"GNd7txm0Rzy_","colab_type":"text"},"source":["## Доля информации, которая содержится в первых $k$ главных компонентах, вычисляется по следующей формуле:"]},{"cell_type":"markdown","metadata":{"id":"yZevVJK8RWNa","colab_type":"text"},"source":["$$\n","\\alpha_k = \\frac{\\sigma_1^2 + ... + \\sigma_k^2}{\\sum_{i=1}^n \\sigma_i^2}, \\ \\ k < n\n","$$"]},{"cell_type":"markdown","metadata":{"id":"gEPNXNoAR97F","colab_type":"text"},"source":["## Если в первых $N$ компонентах содержится практически вся исходня информация (>90%), то это будет означать, что эффективная размерность данных равна $N$. \n","\n","## То есть можно оставить только эти первые $N$ компонент, а остальные можно отбросить"]},{"cell_type":"markdown","metadata":{"id":"d2qUO0NmV4eW","colab_type":"text"},"source":["Эффективную размерность данных можно узнать из графика распределения объясненной дисперсии по главным компонентам\n","\n","Это называется \"Критерий крутого склона\"\n","\n","![alt text](https://i.ibb.co/VJDmXvh/SVslope.jpg)"]},{"cell_type":"markdown","metadata":{"id":"WdQMOe4zSm11","colab_type":"text"},"source":["### Посмотрим на значение сингулярных чисел для датасета с лицами"]},{"cell_type":"code","metadata":{"id":"vL7GKm7_PvDE","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10,8))\n","plt.plot(pca_reduction.explained_variance_, 'C1o')\n","plt.plot(pca_reduction.explained_variance_ratio_)\n","plt.title(\"Распределение сохрененной информации в главных компонентах\")\n","plt.xlabel(\"номер главной компоненты\", fontsize=15)\n","plt.ylabel(\"Процент объясненной дисперсии\", fontsize=15)\n","plt.grid()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRkd26iMTc1W","colab_type":"text"},"source":["**Задание**\n","\n","Посчитайте, сколько процентов информации содержится в 1, 3, 5, 10, 20 первых главных компонентах"]},{"cell_type":"code","metadata":{"id":"FX4QRBsPTVCY","colab_type":"code","colab":{}},"source":["# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_hfc6-WFAG3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWbb0syuT22y","colab_type":"text"},"source":["Казалось бы, после отбрасывания последних компонент мы теряем информацию об исходных объектах, и вследствие этого должны получить более плохое качество работы алгоритма\n","\n","Но на практике выясняется, что даже после сокращения размерности с помощью PCA **новые признаки дают более высокое качество** классификации"]},{"cell_type":"code","metadata":{"id":"hwH0vJsfEaQe","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","gscv_rf = GridSearchCV(RandomForestClassifier(),\n","                       {'n_estimators': [100, 200, 500, 800], 'max_depth': [2, 3, 4, 5]},\n","                       cv=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZfEfhaOEaQh","colab_type":"code","colab":{}},"source":["%%time\n","gscv_rf.fit(faces_images, faces_ids)\n","print(gscv_rf.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQFF8dVVEaQj","colab_type":"code","colab":{}},"source":["%%time\n","gscv_rf.fit(red.transform(faces_images)[:,:100], faces_ids)\n","print(gscv_rf.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s18dw9arUii7","colab_type":"text"},"source":["Это происходит потому, что главные компоненты ортогональны друг другу, а значит новые признаки после PCA становятся ортогональны друг другу, а значит не коррелируют друг с другом. \n","\n","Именно поэтому PCA еще иногда называют **декоррелирующим преобразованием**\n","\n","Напомню, что алгоритмы машинного обучения сильно страдают от наличия в данных сильно скоррелированных признаков."]},{"cell_type":"markdown","metadata":{"id":"OCOZbPruEaQm","colab_type":"text"},"source":["Во-вторых, их можно использовать для компактного хранения данных. Для этого объекты трансформируются\n","в новое пространство, и из него выкидываются самые незначимые признаки.\n","\n","Ниже приведены результаты сжатия более чем в 10 раз."]},{"cell_type":"code","metadata":{"id":"YyZk9sd_EaQn","colab_type":"code","colab":{}},"source":["base_size = image_shape[0] * image_shape[1]\n","\n","def compress_and_show(compress_ratio):\n","    pca_reduction = PCA(n_components=int(base_size * compress_ratio))\n","    pca_reduction.fit(faces_images)\n","\n","    faces_compressed = pca_reduction.transform(faces_images)\n","    faces_restored = pca_reduction.inverse_transform(faces_compressed) + mean_face\n","\n","    plt.figure(figsize=(16, 8))\n","    rows, cols = 2, 4\n","    n_samples = rows * cols\n","    for i in range(n_samples):\n","        plt.subplot(rows, cols, i + 1)\n","        plt.imshow(faces_restored[i, :].reshape(image_shape), interpolation='none',\n","                   cmap='gray')\n","        plt.xticks(())\n","        plt.yticks(())\n","        \n","compress_and_show(0.08)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZrs4pquEaQq","colab_type":"text"},"source":["И даже при сжатии в 20 раз лица остаются узнаваемыми."]},{"cell_type":"code","metadata":{"id":"P0O0a0MeEaQq","colab_type":"code","colab":{}},"source":["compress_and_show(0.05)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckSYwaSsFNXA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXrYTWQkL-P0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcVCv5ORL-Sb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zk2E70xZL-VE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVsIPoyKL-Xa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qizfNS5KL-Zx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAAVLVF4L-cc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hXfi6BIL-ev","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHGJWnGvL-hO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SISRLwW6L-jl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tlp5r7mDL-mD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ap100R_8L-qy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_Ign-u7L-tb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yq1Z0ywL-vw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-Luk6qOL-ya","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mc-oS_rpL-06","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5yQ3d-VL-3Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYNZFg-KL-5-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGBWfh0-L_Fw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vf6LhV8jL_JT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmZMRh1HL_C4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQ7LM3eEL_AY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPp1HBBcH3t6","colab_type":"text"},"source":["**BONUS**\n","\n","Визуализация работы **t-SNE**"]},{"cell_type":"markdown","metadata":{"id":"ZfQFWZiaH3yT","colab_type":"text"},"source":["![alt text](https://github.com/KellerJordan/figures/raw/master/mnist70k-tsne.gif)"]},{"cell_type":"code","metadata":{"id":"j1nd4_IQH5Xb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mtJHYkYvKpxw","colab_type":"text"},"source":["## Доступное объяснение того, как работает t-SNE, на [этом сайте](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1) и еще [на этом ресурсе](https://distill.pub/2016/misread-tsne/)"]},{"cell_type":"markdown","metadata":{"id":"WKbKOS7tI4GP","colab_type":"text"},"source":["## Еще хорошие визуализации находятся в [этом репозитории](https://github.com/sophronesis/tsne_animate)\n","\n","## Посмотреть, как t-SNE преобразует данные с разной геометрией можно [по этой ссылке](https://distill.pub/2016/misread-tsne/#perplexity=10&epsilon=5&demo=1&demoParams=50,2)"]},{"cell_type":"code","metadata":{"id":"xtyJgFy4I_J-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}